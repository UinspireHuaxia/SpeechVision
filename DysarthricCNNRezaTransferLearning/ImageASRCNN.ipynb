{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reza/anaconda3/envs/SpeechVisionENV/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/reza/anaconda3/envs/SpeechVisionENV/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 155\n"
     ]
    }
   ],
   "source": [
    "import utilities\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout , SpatialDropout2D\n",
    "from keras.layers import AveragePooling2D, Activation\n",
    "from keras.callbacks import History\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "from  keras import optimizers\n",
    "from keras import losses\n",
    "from keras.regularizers import l1_l2, l1,l2\n",
    "from keras.models import model_from_json\n",
    "from sklearn.utils import class_weight\n",
    "import pyprog\n",
    "#print (tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "SETTINGS_DIR = os.path.dirname(os.path.realpath('__file__'))\n",
    "train_set_path = SETTINGS_DIR+'/images/Control/Train/'\n",
    "\n",
    "#test_set_path = SETTINGS_DIR+'/images/Dysarthric/Test/Combined'\n",
    "test_set_path = SETTINGS_DIR+\"/images/Control/Test/\"\n",
    "#test_set_path = SETTINGS_DIR+\"/images/Dysarthric/Test/F05\"\n",
    "#test_set_path = SETTINGS_DIR+\"/images/Dysarthric/Test/M06\"\n",
    "#test_set_path = SETTINGS_DIR+\"/images/Dysarthric/Test/M10\"\n",
    "\n",
    "dnn_file_name_structure = SETTINGS_DIR +\"/cnn_control.json\"\n",
    "training_dynamics_path = SETTINGS_DIR+'/TrainingDynamics.csv'\n",
    "dnn_file_name_weights = SETTINGS_DIR +  \"/cnn_weight_control.h5\"\n",
    "\n",
    "\n",
    "# IMPORTANT: Deleting .ipynb_checkpoints from image paths\n",
    "!find '.' -name '*.ipynb_checkpoints' -exec rm -r {} +\n",
    "\n",
    "batch_size=128\n",
    "image_input_size=(150,150)\n",
    "vocab_size = utilities.get_no_folders_in_path(test_set_path)\n",
    "print (\"Vocabulary Size:\",vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile(model):\n",
    "    model.compile(loss=losses.categorical_crossentropy,\n",
    "                          optimizer=optimizers.Adadelta(),\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "def get_model():\n",
    "    \n",
    "    #droprate=0.4\n",
    "    droprate=0.5\n",
    "\n",
    "    classifier = Sequential()\n",
    "\n",
    "    classifier.add( Convolution2D(  filters=32, kernel_size=(3,3), \n",
    "                                  input_shape= (*image_input_size,3), \n",
    "                                  activation='relu')  )\n",
    "    classifier.add( SpatialDropout2D(droprate) )\n",
    "    classifier.add( Convolution2D(  filters=32, kernel_size=(3,3),activation='relu'  )  )\n",
    "    classifier.add(MaxPooling2D (pool_size=(2,2) ) )\n",
    "    #classifier.add(Dropout(droprate))\n",
    "\n",
    "\n",
    "    classifier.add( Convolution2D(  filters=64, kernel_size=(3,3),activation='relu'  )  )\n",
    "    classifier.add( SpatialDropout2D(droprate) )\n",
    "    classifier.add( Convolution2D(  filters=64, kernel_size=(3,3),activation='relu'  )  )\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPooling2D (pool_size=(2,2) ) )\n",
    "    #classifier.add(Dropout(droprate))\n",
    "\n",
    "    classifier.add( Convolution2D(  filters=128, kernel_size=(3,3),activation='relu'  )  )\n",
    "    classifier.add( SpatialDropout2D(droprate) )\n",
    "    classifier.add( Convolution2D(  filters=128, kernel_size=(3,3),activation='relu'  )  )\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPooling2D (pool_size=(2,2) ) )\n",
    "    #classifier.add(Dropout(droprate))\n",
    "\n",
    "    classifier.add( Convolution2D(  filters=256, kernel_size=(3,3),activation='relu'  )  )\n",
    "    classifier.add( SpatialDropout2D(droprate) )\n",
    "    classifier.add( Convolution2D(  filters=256, kernel_size=(3,3),activation='relu'  )  )\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPooling2D (pool_size=(2,2) ) )\n",
    "    #classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dropout(droprate))\n",
    "\n",
    "    classifier.add (Flatten( ) )\n",
    "\n",
    "    classifier.add(Dense (units=vocab_size, activation='softmax' ))\n",
    "    classifier.summary()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_an_image(image_path, model):\n",
    "    \n",
    "    from tensorflow.keras.preprocessing import image\n",
    "\n",
    "    test_image = image.load_img(image_path, target_size = image_input_size)\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0) \n",
    "\n",
    "    y_pred = model.predict_classes(test_image,batch_size)[0]\n",
    "    classes =training_set.class_indices\n",
    "    for key, value in classes.items():\n",
    "        if value==y_pred:\n",
    "            break       \n",
    "\n",
    "    pred_key=utilities.dictionary .index [ utilities.dictionary  ['FILE NAME'] == key ] \n",
    "    predicted_word=utilities.dictionary .iloc[pred_key[0],0]\n",
    "    # Get true label\n",
    "    true_key=true_key=utilities.file_to_index(image_path)\n",
    "    true_word = utilities.dictionary .iloc[true_key,0]\n",
    "    #print(\"Predicted:\",predicted_word,\", True:\",true_word)\n",
    "    return predicted_word, true_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epoch():\n",
    "    if os.path.exists(training_dynamics_path):\n",
    "        training_dynamics=pd.read_csv(training_dynamics_path)\n",
    "        return training_dynamics[\"Epoch\"][len(training_dynamics)-1]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def load_model(learning_rate=0.001):\n",
    "    # Loading the CNN\n",
    "    json_file = open(dnn_file_name_structure, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(dnn_file_name_weights)\n",
    "    model_compile(model)\n",
    "    return model\n",
    "\n",
    "def save_model(model):\n",
    "    # Save/overwrite the model\n",
    "    model_json = model.to_json()\n",
    "    with open(dnn_file_name_structure, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(dnn_file_name_weights)\n",
    "    \n",
    "def save_training_dynamics(epoch,history,with_header=False):\n",
    "    training_dynamics=pd.DataFrame(\n",
    "        data = [ [epoch, history.history['loss'][0] ,  history.history['acc'][0],  \n",
    "                history.history['val_loss'][0],  history.history['val_acc'][0] ]],\n",
    "        columns=[\"Epoch\",\"TrainingLoss\", \"TrainingAccuracy\",\"ValidationLoss\",\"ValidationAccuracy\"]\n",
    "    )\n",
    "    if (with_header):\n",
    "        with open(training_dynamics_path, 'a') as csv_file:\n",
    "            training_dynamics.to_csv(csv_file, header=True)\n",
    "    else:\n",
    "        with open(training_dynamics_path, 'a') as csv_file:\n",
    "            training_dynamics.to_csv(csv_file, header=False)\n",
    "            \n",
    "def visualize_training():\n",
    "    import matplotlib.pyplot as plt\n",
    "    training_dynamics=pd.read_csv(training_dynamics_path)\n",
    "    loss_values = training_dynamics[\"TrainingLoss\"]\n",
    "    val_loss_values = training_dynamics[\"ValidationLoss\"]\n",
    "    epochs = range(1, len (training_dynamics['Epoch'])+1)\n",
    "    plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'g.-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Ploting Accuracy\n",
    "    loss_values = training_dynamics[\"TrainingAccuracy\"]\n",
    "    val_loss_values = training_dynamics[\"ValidationAccuracy\"]\n",
    "    epochs = range(1, len (training_dynamics['Epoch'])+1)\n",
    "    plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'g.-', label='Validation loss')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets():\n",
    "        from keras.preprocessing.image import ImageDataGenerator\n",
    "        \n",
    "        # https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "            width_shift_range=0.30,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            fill_mode='nearest',\n",
    "            horizontal_flip=False)\n",
    "        \n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        # If shuffle=False then the validation results will be different from classifier.predict_generator()\n",
    "        training_set = train_datagen.flow_from_directory(\n",
    "            train_set_path,\n",
    "            target_size=image_input_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True)\n",
    "        \n",
    "        test_set = test_datagen.flow_from_directory(\n",
    "           test_set_path,\n",
    "            target_size=image_input_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "        return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(test_set_generator):\n",
    "    steps=test_set_generator.samples/batch_size\n",
    "    model = load_model()\n",
    "\n",
    "    y_pred = model.evaluate_generator(test_set_generator, steps = steps, verbose = 1)\n",
    "    y_test = test_set_generator.classes\n",
    "    correct_classifications=0\n",
    "    for pred,label in zip(y_pred, y_test):\n",
    "        if pred.argmax()==label:\n",
    "            correct_classifications+=1\n",
    "    #print (\"Testing acuracy:\", correct_classifications/len(y_pred) *100,\"%\")\n",
    "    print (\"Testing acuracy:\", y_pred[1] *100,\"%\")\n",
    "    return \n",
    "\n",
    "def manual_testing():\n",
    "    model = load_model() \n",
    "    #test_path = SETTINGS_DIR+\"/images/Control/Test\"\n",
    "    #test_path = SETTINGS_DIR+\"/images/Dysarthric/Test/F05\"\n",
    "    #test_path = SETTINGS_DIR+\"/images/Dysarthric/Test/M06\"\n",
    "    #test_path = SETTINGS_DIR+\"/images/Dysarthric/Test/M10\"\n",
    "    #test_path = SETTINGS_DIR+\"/images/Control/Test\"\n",
    "    test_path = test_set_path\n",
    "    \n",
    "    correct_classifications=0\n",
    "    i=0\n",
    "    prog = pyprog.ProgressBar(\"Predicting \", \" Done\", utilities.get_no_files_in_path(test_path))\n",
    "    # Show the initial status\n",
    "    prog.update()\n",
    "    no_processed=i\n",
    "    for directory, s, files in os.walk(test_path):\n",
    "            for f in files:\n",
    "                file_path=directory+\"/\"+f\n",
    "                if (\"jpg\" in f):                \n",
    "                    predicted_word, true_word = predict_an_image(file_path,model)\n",
    "                    #print (predicted_word,true_word)\n",
    "                    if (predicted_word==true_word):\n",
    "                        correct_classifications+=1\n",
    "                    i+=1\n",
    "                    prog.set_stat(i)\n",
    "                    prog.update()\n",
    "\n",
    "    prog.end()\n",
    "    print (\"Testing acuracy:\", correct_classifications/i *100,\"%\")\n",
    "    return (correct_classifications/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ideal_loss=0.01, is_dnn_structure_changned=False,\n",
    "              learning_rate=0.001,  max_epoch=50, enabled_trasfer_learning=False):\n",
    "        \n",
    "        is_new_dnn=False\n",
    "        history = History()\n",
    "        \n",
    "        print(\"=================================================\")\n",
    "        \n",
    "        if (os.path.isfile(dnn_file_name_structure) and\n",
    "                (os.path.isfile(dnn_file_name_weights)) and \n",
    "                (is_dnn_structure_changned == False)):\n",
    "            # load the previosly trained DNN\n",
    "            if (enabled_trasfer_learning):\n",
    "                # Enable Transfer Learning\n",
    "                print (\"Transfer learning is enabled.\")\n",
    "                model = FreezeLayers(load_model(),top_unfrozen_layer_name=\"conv2d_5\" ) \n",
    "            else:\n",
    "                print (\"Transer learning is disabled.\")\n",
    "                model = load_model(learning_rate=learning_rate)\n",
    "            print(\"CNN is loaded.\")\n",
    "        else:\n",
    "            # Create a new model\n",
    "            model =  get_model()                    \n",
    "            print(\"CNN is created\")\n",
    "            # Erase the training_dynamic_csv file\n",
    "            if os.path.exists(training_dynamics_path):\n",
    "                os.remove(training_dynamics_path)\n",
    "            is_new_dnn=True\n",
    "            model_compile(model)\n",
    "        \n",
    "        ep= read_epoch()+1\n",
    "        PringFrozenLayers(model)\n",
    "        model.fit_generator(\n",
    "            training_set,\n",
    "            steps_per_epoch=training_set.samples/batch_size,\n",
    "            #steps_per_epoch=500,\n",
    "                             epochs=1,\n",
    "                           \n",
    "                             validation_data=test_set,\n",
    "                             validation_steps=test_set.samples/batch_size,\n",
    "                             workers=10, \n",
    "                             max_queue_size=10,  callbacks=[history])\n",
    "        save_training_dynamics(ep,history,with_header=is_new_dnn)\n",
    "       \n",
    "        while (history.history['loss'][0] >= ideal_loss):\n",
    "            print(\"Epoch\", ep)\n",
    "            model.fit_generator(\n",
    "            training_set,\n",
    "            steps_per_epoch=training_set.samples/batch_size,\n",
    "            #steps_per_epoch=500,\n",
    "                             epochs=1,\n",
    "                             \n",
    "                             validation_data=test_set,\n",
    "                             validation_steps=test_set.samples/batch_size,\n",
    "                             workers=10,\n",
    "                             max_queue_size=10,  callbacks=[history])\n",
    "\n",
    "            # Save/overwrite the model\n",
    "            save_model(model)\n",
    "            \n",
    "            if (ep%5==0):\n",
    "                if(manual_testing()>0.85):\n",
    "                    break\n",
    "                \n",
    "            if   (history.history['val_acc'][0]>0.88):\n",
    "                break\n",
    "                \n",
    "            ep += 1\n",
    "            save_training_dynamics(ep,history,with_header=False)        \n",
    "\n",
    "            # stop the traning if certain training accuracy is reached\n",
    "            if (history.history['loss'][0]<ideal_loss):\n",
    "                   break\n",
    "            if (ep > max_epoch):\n",
    "                break\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning: freeze top layers but unfreeze all layers below the given layer\n",
    "def FreezeLayers(model, top_unfrozen_layer_name):\n",
    "    model.trainable=True\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if (layer.name==top_unfrozen_layer_name):\n",
    "            set_trainable=True\n",
    "        if (set_trainable):\n",
    "            layer.trainable=True\n",
    "        else:\n",
    "            layer.trainable=False\n",
    "    model_compile(model)\n",
    "    return model\n",
    "\n",
    "def PringFrozenLayers(model):\n",
    "     for layer in model.layers:\n",
    "            print (\"Layer:\",layer.name, \"Frozen:\",not layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15736 images belonging to 155 classes.\n",
      "Found 1395 images belonging to 155 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load X and y\n",
    "training_set, test_set =get_train_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train       \n",
    "history = train(is_dnn_structure_changned= False,  \n",
    "                enabled_trasfer_learning=False,max_epoch=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred=test_generator(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "11/10 [==============================] - 13s 1s/step\n",
      "Testing acuracy: 84.08602134301245 %\n"
     ]
    }
   ],
   "source": [
    "y_pred=test_generator(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a single prediction\n",
    "model = load_model() \n",
    "image_path = SETTINGS_DIR+\"/images/Control/Test/C10/CM06_B1_C10_M3.jpg\"\n",
    "predicted_word, true_word = predict_an_image(image_path,model)\n",
    "print (\"Prediction:\", predicted_word, \" --- True Label:\", true_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Progress: 100% ##################################################  Done\n",
      "Testing acuracy: 72.68817204301075 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7268817204301076"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Prediction\n",
    "manual_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "for directory, s, files in os.walk(SETTINGS_DIR+\"/images/Dysarthric/Train\"):\n",
    "        for f in files:\n",
    "            file_path=directory+\"/\"+f\n",
    "            if (\"M01_\" in f): \n",
    "                os.remove(file_path)\n",
    "                print (file_path,\"is removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq=Sequential()\n",
    "model = load_model()\n",
    "\n",
    "for layer in model.layers[:-3]:\n",
    "    seq.add(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
